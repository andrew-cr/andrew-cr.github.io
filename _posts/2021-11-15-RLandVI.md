---
title: 'Using Reinforcement Learning for Variational Inference'
permalink: /posts/RLandVI/
tags:
---

This blog post details how we can use ideas from Reinforcement Learning (RL) to
efficiently perform variational inference in sequential settings.

# Background

## Sequential Variational Inference

I will depart from the RL setting for a while to introduce sequential variational inference. We will be modelling sequential data in the form of a series of observations $$y_1, y_2, \dots, y_T$$. We assume that there is some hidden true state of the world $$x_t$$ that is producing these observations and undergoing some form of transition between each time step. This model can be summarized with an observation distribution

$$y_t \vert x_t \sim g(y_t \vert x_t)$$

and a transition distribution

$$ x_{t+1} \vert x_t \sim f(x_{t+1} \vert x_t)  $$

This defines a joint distribution over hidden states and observations

$$ p(x_{1:T}, y_{1:T}) = \prod_{t=1}^T f(x_t \vert x_{t-1}) g(y_t \vert x_t)$$

We would like to make inferences about the hidden state given we have seen the observations $$y_1, y_2, \dots, y_T = y_{1:T}$$. That is, we would like to approximate the posterior distribution

$$ p(x_{1:T} \vert y_{1:T})$$

This posterior is intractable to compute directly in most cases which is where variational inference comes in handy. Using the variational inference framework, we create a variational distribution $$q^\phi(x_{1:T})$$ with parameters $$\phi$$ that will approximate this true posterior distribution for a given set of observations $$y_{1:T}$$. We perform this approximation by maximizing the evidence lower bound (ELBO) with respect to $$\phi$$

$$ \text{max} \quad \text{ELBO}_T $$

$$  = \text{max} \quad \log p(y_{1:T}) - \text{KL} \Bigg(  q^\phi(x_{1:T}) \vert \vert p(x_{1:T} \vert y_{1:T}) \Bigg)$$

$$  = \text{max} \quad \mathbb{E}_{q^\phi(x_{1:T})} \left[ \log \frac{p(x_{1:T}, y_{1:T})}{q^\phi(x_{1:T})}\right] $$


This corresponds to posterior approximation as the data log likelihood $$\log p(y_{1:T})$$ does not depend on the variational parameters $$\phi$$ so maximizing the ELBO corresponds to minimizing the KL divergence between the variational distribution and the true posterior.

## Reinforcement Learning

I first briefly go through the basic setting of RL.

We encode all the relevant information about environment in which the agent is 
acting into a *state vector* $$s_t$$ which will evolve as the agent interacts
with the environment. According to the current state, the agent selects an *action*
$$a_t$$ from its policy $$\pi(a_t | s_t)$$. After taking this action, the environment
will then evolve according to a transition distribution $$P(s_{t+1} | s_t, a_t)$$
which depends on both the current state and the action the agent decided to take.

The repeated choice of actions and state transitions can be summarized in the following diagram:

![Graphical model of states and actions](/images/states and actions.png)

The agent is not just taking random actions, but its intent is to take actions such
as to maximize a *reward* $$r(s_t, a_t)$$. The agent recieves a reward after each
state-action choice. In the setting where the agent interacts with the environment
for a total of $$T$$ steps, then we would like to choose a policy $$\pi(a_t | s_t)$$
such that the agent maximizes the total reward received:

$$J(\phi) = \mathbb{E}_{\tau \sim p_\phi} \left[ \sum_{t=1}^T r(s_t, a_t) \right]$$

This expectation is taken with respect to the trajectory distribution which is
the distribution over all states and actions induced by the chosen policy $$\pi_\phi$$

$$ p_\phi(\tau) = P(s_1) \pi_\phi(a_1 | s_1) \prod_{t=2}^T P(s_t | s_{t-1}, a_{t-1}) \pi_\phi(a_t|s_t)$$


Note how we have parameterized the policy, $$ \pi_\phi(a_t \vert s_t) $$ with parameters $$\phi$$.

To help choose good policies, RL uses ideas from dynamic programming, where a *value function* $$V_t(s_t)$$ is introduced which summarizes the expected future reward if the agent were to find itself in state $$s_t$$ at time $$t$$. This gives a level of foresight which can be used to pick good actions at the current time step $$t$$. $$V_t(s_t)$$ is defined as

$$ V_t(s_t) = \mathbb{E} \left[ \sum_{k=t}^T r(s_k, a_k) \right]  $$

From this definition, a recursion for the value function can be derived

$$ V_t(s_t) = \mathbb{E}_{s_{t+1}, a_t \sim P(s_{t+1} | s_t, a_t) \pi_\phi(a_t | s_t)} \left[ r(s_t, a_t) + V_{t+1}(s_{t+1}) \right]  $$

This is called the Bellman recursion.


# From a backwards recursion to a forwards recursion

The Bellman recursion introduced in the previous section operates in the backwards direction. That is, the value function at time $$t$$ depends on the current reward and the future value function at the next step, $$t+1$$. Our eventual aim is to apply these ideas to variational inference in sequential settings where we only move forwards in time. This requires a forwards recursion where a 'value function' will depend on the past value function at the previous time step.
